<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>dataprovider API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dataprovider</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import List, Set, Optional

from feature import Key, Timestamp, Feature
from datacheck import DataCheck
from environment import EnvironmentType
from common import Tag, Metadata


class DataProviderType:
    &#34;&#34;&#34;
    What is the actual data infra?  BigQuery, Snowflake, S3, Google Sheests, Kafka, etc
    &#34;&#34;&#34;

    # TODO: implement this


class InputDataSchema:
    &#34;&#34;&#34;
    `InputDataSchema` defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.

    `InputDataProviders` defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by `InputDataSchema`.

    Together, `InputDataSchema` and `InputDataProviders` are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).

    Why separate `InputDataSchema` from the `InputDataProviders`?

    Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).

    Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.

    The `InputDataSchema` abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.
    &#34;&#34;&#34;

    metadata: Metadata
    &#34;&#34;&#34;
    name, description, key:value tags
    &#34;&#34;&#34;

    keys: List[Key]
    &#34;&#34;&#34;
    The unique keys that describe the entity contained within this table.  For example, [user_id, transaction_id}.
    Often referred to as:
    * Primary key
    * Data grain
    * Entity
    * ... others ...
    &#34;&#34;&#34;

    # TODO: Other feature stores split out entity to a top level object.  Why is this?  I keep thinking it feels redundant but maybe there is an advantage to having shared entities when it comes to collaboration particularly when teams are sharing features about many common entities such as users?

    timestamp: Timestamp
    &#34;&#34;&#34;
    Which column of data has the timestamp of the record?
    Used to support time-travel-aware joins.
    &#34;&#34;&#34;

    output_features: List[Feature]
    &#34;&#34;&#34;
    The `Features` delivered by a InputDataProvider confirming to this `InputDataSchema`.
    
    In database terms, the schema.

    `keys` and `timestamp` can be treated by the user as if they were defined as output_features.  Orchestra automatically appends these Features to the list.
    &#34;&#34;&#34;

    data_checks: List[DataCheck]
    &#34;&#34;&#34;
    Any data quality or data distribution checks that should be performed on the incoming data.  Executed by Orchesrta using the user&#39;s supplied checking framwork.
    &#34;&#34;&#34;


class DataProviderConfig:
    &#34;&#34;&#34;
    Configuration settings for a specific provider
    For example
    # [1] S3 path + credentials
    # [2] BigQuery database URI + credentials
    # [3] DBT model directory + run command
    &#34;&#34;&#34;

    # TODO: implement this to be key:value pairs


class InputDataSource:
    &#34;&#34;&#34;
    `InputDataSchema` defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.

    `InputDataProviders` defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by `InputDataSchema`.

    Together, `InputDataSchema` and `InputDataProviders` are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).

    Why separate `InputDataSchema` from the `InputDataProviders`?

    Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).

    Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.

    The `InputDataSchema` abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.
    &#34;&#34;&#34;

    metadata: Metadata
    &#34;&#34;&#34;
    name, description, key:value tags
    &#34;&#34;&#34;

    provider: DataProviderType
    &#34;&#34;&#34;
    What is the actual infra?  BigQuery, Snowflake, S3, Google Sheests, Kafka, etc
    &#34;&#34;&#34;

    provider_config: DataProviderConfig
    &#34;&#34;&#34;
    Configuration settings for a specific provider
    For example:
    [1] S3 path + credentials
    [2] BigQuery database URI + credentials
    [3] DBT model directory + run command&#34;&#34;&#34;

    environment: Set[EnvironmentType]
    &#34;&#34;&#34;
    In which environments is this data available for use?  Use cases:
    [1] Have seperate DatasetProviders in development vs. production (e.g,. use a local CSV export of sensitive data that&#39;s only available with machine-level credentials, etc)
    [2] Have multiple production DatasetProviders e.g., Kafka + Snowflake where one provider is used for second- latency features and the other used for week+ latency features.
    &#34;&#34;&#34;

    # TODO: Do we need a way to run DataCode() on a InputDataProvider?  or, should this be handled by each provider&#39;s config?  An example use case might be changing column names, dropping rows that are &gt;x% null, filtering out known bad rows, etc.  I lean to yes...


class OutputDataDestination:
    &#34;&#34;&#34; &#34;&#34;&#34;

    # TODO: Add in the concept of a data sink.  How will we save things like prediction logs, cached training data, etc.</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dataprovider.DataProviderConfig"><code class="flex name class">
<span>class <span class="ident">DataProviderConfig</span></span>
</code></dt>
<dd>
<div class="desc"><p>Configuration settings for a specific provider
For example</p>
<h1 id="1-s3-path-credentials">[1] S3 path + credentials</h1>
<h1 id="2-bigquery-database-uri-credentials">[2] BigQuery database URI + credentials</h1>
<h1 id="3-dbt-model-directory-run-command">[3] DBT model directory + run command</h1></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataProviderConfig:
    &#34;&#34;&#34;
    Configuration settings for a specific provider
    For example
    # [1] S3 path + credentials
    # [2] BigQuery database URI + credentials
    # [3] DBT model directory + run command
    &#34;&#34;&#34;

    # TODO: implement this to be key:value pairs</code></pre>
</details>
</dd>
<dt id="dataprovider.DataProviderType"><code class="flex name class">
<span>class <span class="ident">DataProviderType</span></span>
</code></dt>
<dd>
<div class="desc"><p>What is the actual data infra?
BigQuery, Snowflake, S3, Google Sheests, Kafka, etc</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataProviderType:
    &#34;&#34;&#34;
    What is the actual data infra?  BigQuery, Snowflake, S3, Google Sheests, Kafka, etc
    &#34;&#34;&#34;

    # TODO: implement this</code></pre>
</details>
</dd>
<dt id="dataprovider.InputDataSchema"><code class="flex name class">
<span>class <span class="ident">InputDataSchema</span></span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.</p>
<p><code>InputDataProviders</code> defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code>.</p>
<p>Together, <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> and <code>InputDataProviders</code> are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).</p>
<p>Why separate <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> from the <code>InputDataProviders</code>?</p>
<p>Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).</p>
<p>Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.</p>
<p>The <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InputDataSchema:
    &#34;&#34;&#34;
    `InputDataSchema` defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.

    `InputDataProviders` defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by `InputDataSchema`.

    Together, `InputDataSchema` and `InputDataProviders` are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).

    Why separate `InputDataSchema` from the `InputDataProviders`?

    Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).

    Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.

    The `InputDataSchema` abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.
    &#34;&#34;&#34;

    metadata: Metadata
    &#34;&#34;&#34;
    name, description, key:value tags
    &#34;&#34;&#34;

    keys: List[Key]
    &#34;&#34;&#34;
    The unique keys that describe the entity contained within this table.  For example, [user_id, transaction_id}.
    Often referred to as:
    * Primary key
    * Data grain
    * Entity
    * ... others ...
    &#34;&#34;&#34;

    # TODO: Other feature stores split out entity to a top level object.  Why is this?  I keep thinking it feels redundant but maybe there is an advantage to having shared entities when it comes to collaboration particularly when teams are sharing features about many common entities such as users?

    timestamp: Timestamp
    &#34;&#34;&#34;
    Which column of data has the timestamp of the record?
    Used to support time-travel-aware joins.
    &#34;&#34;&#34;

    output_features: List[Feature]
    &#34;&#34;&#34;
    The `Features` delivered by a InputDataProvider confirming to this `InputDataSchema`.
    
    In database terms, the schema.

    `keys` and `timestamp` can be treated by the user as if they were defined as output_features.  Orchestra automatically appends these Features to the list.
    &#34;&#34;&#34;

    data_checks: List[DataCheck]
    &#34;&#34;&#34;
    Any data quality or data distribution checks that should be performed on the incoming data.  Executed by Orchesrta using the user&#39;s supplied checking framwork.
    &#34;&#34;&#34;</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dataprovider.InputDataSchema.data_checks"><code class="name">var <span class="ident">data_checks</span> : List[<a title="datacheck.DataCheck" href="datacheck.html#datacheck.DataCheck">DataCheck</a>]</code></dt>
<dd>
<div class="desc"><p>Any data quality or data distribution checks that should be performed on the incoming data.
Executed by Orchesrta using the user's supplied checking framwork.</p></div>
</dd>
<dt id="dataprovider.InputDataSchema.keys"><code class="name">var <span class="ident">keys</span> : List[<a title="feature.Key" href="feature.html#feature.Key">Key</a>]</code></dt>
<dd>
<div class="desc"><p>The unique keys that describe the entity contained within this table.
For example, [user_id, transaction_id}.
Often referred to as:
* Primary key
* Data grain
* Entity
* &hellip; others &hellip;</p></div>
</dd>
<dt id="dataprovider.InputDataSchema.metadata"><code class="name">var <span class="ident">metadata</span> : <a title="common.Metadata" href="common.html#common.Metadata">Metadata</a></code></dt>
<dd>
<div class="desc"><p>name, description, key:value tags</p></div>
</dd>
<dt id="dataprovider.InputDataSchema.output_features"><code class="name">var <span class="ident">output_features</span> : List[<a title="feature.Feature" href="feature.html#feature.Feature">Feature</a>]</code></dt>
<dd>
<div class="desc"><p>The <code>Features</code> delivered by a InputDataProvider confirming to this <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code>.</p>
<p>In database terms, the schema.</p>
<p><code>keys</code> and <code>timestamp</code> can be treated by the user as if they were defined as output_features.
Orchestra automatically appends these Features to the list.</p></div>
</dd>
<dt id="dataprovider.InputDataSchema.timestamp"><code class="name">var <span class="ident">timestamp</span> : <a title="feature.Timestamp" href="feature.html#feature.Timestamp">Timestamp</a></code></dt>
<dd>
<div class="desc"><p>Which column of data has the timestamp of the record?
Used to support time-travel-aware joins.</p></div>
</dd>
</dl>
</dd>
<dt id="dataprovider.InputDataSource"><code class="flex name class">
<span>class <span class="ident">InputDataSource</span></span>
</code></dt>
<dd>
<div class="desc"><p><code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.</p>
<p><code>InputDataProviders</code> defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code>.</p>
<p>Together, <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> and <code>InputDataProviders</code> are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).</p>
<p>Why separate <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> from the <code>InputDataProviders</code>?</p>
<p>Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).</p>
<p>Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.</p>
<p>The <code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code> abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InputDataSource:
    &#34;&#34;&#34;
    `InputDataSchema` defines the schema of data (aka RawFeatures) ingested by data scientists to further manipulate into DerivedFeatures that a Model can process.

    `InputDataProviders` defines an actual data source (e.g., database, s3/blob store bucket, kafka streem, google sheet, etc) that provides data in the schema defined by `InputDataSchema`.

    Together, `InputDataSchema` and `InputDataProviders` are designed to provide a clean abstraction between the upstream data sources (that data scientists have little to no control over, but are extremely dependent upon) and the feature engineering work (that data scientists have complete control over).

    Why separate `InputDataSchema` from the `InputDataProviders`?

    Particularly when building user-facing models, it is common to have training data come from a one data provider (e.g., an analytical warehouse or data lake with historical data) and production data (that is fed to the model for a prediction) come from a combination of data providers (e.g., features requiring a week+ freshness are computed from analytical data sources and cached while features requiring a second- freshness are computed in near real time from a streaming data source).

    Of course, for the model to actually work, these multiple data sources must deliver the same data schema, hence the abstraction that supports a specific definition.

    The `InputDataSchema` abstraction enables Orchestra to intelligently orchestrate your data infrastructure, for example, applying schema over an unstructured Kafka stream or alerting when data quality checks fail on one source.
    &#34;&#34;&#34;

    metadata: Metadata
    &#34;&#34;&#34;
    name, description, key:value tags
    &#34;&#34;&#34;

    provider: DataProviderType
    &#34;&#34;&#34;
    What is the actual infra?  BigQuery, Snowflake, S3, Google Sheests, Kafka, etc
    &#34;&#34;&#34;

    provider_config: DataProviderConfig
    &#34;&#34;&#34;
    Configuration settings for a specific provider
    For example:
    [1] S3 path + credentials
    [2] BigQuery database URI + credentials
    [3] DBT model directory + run command&#34;&#34;&#34;

    environment: Set[EnvironmentType]
    &#34;&#34;&#34;
    In which environments is this data available for use?  Use cases:
    [1] Have seperate DatasetProviders in development vs. production (e.g,. use a local CSV export of sensitive data that&#39;s only available with machine-level credentials, etc)
    [2] Have multiple production DatasetProviders e.g., Kafka + Snowflake where one provider is used for second- latency features and the other used for week+ latency features.
    &#34;&#34;&#34;

    # TODO: Do we need a way to run DataCode() on a InputDataProvider?  or, should this be handled by each provider&#39;s config?  An example use case might be changing column names, dropping rows that are &gt;x% null, filtering out known bad rows, etc.  I lean to yes...</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="dataprovider.InputDataSource.environment"><code class="name">var <span class="ident">environment</span> : Set[<a title="environment.EnvironmentType" href="environment.html#environment.EnvironmentType">EnvironmentType</a>]</code></dt>
<dd>
<div class="desc"><p>In which environments is this data available for use?
Use cases:
[1] Have seperate DatasetProviders in development vs. production (e.g,. use a local CSV export of sensitive data that's only available with machine-level credentials, etc)
[2] Have multiple production DatasetProviders e.g., Kafka + Snowflake where one provider is used for second- latency features and the other used for week+ latency features.</p></div>
</dd>
<dt id="dataprovider.InputDataSource.metadata"><code class="name">var <span class="ident">metadata</span> : <a title="common.Metadata" href="common.html#common.Metadata">Metadata</a></code></dt>
<dd>
<div class="desc"><p>name, description, key:value tags</p></div>
</dd>
<dt id="dataprovider.InputDataSource.provider"><code class="name">var <span class="ident">provider</span> : <a title="dataprovider.DataProviderType" href="#dataprovider.DataProviderType">DataProviderType</a></code></dt>
<dd>
<div class="desc"><p>What is the actual infra?
BigQuery, Snowflake, S3, Google Sheests, Kafka, etc</p></div>
</dd>
<dt id="dataprovider.InputDataSource.provider_config"><code class="name">var <span class="ident">provider_config</span> : <a title="dataprovider.DataProviderConfig" href="#dataprovider.DataProviderConfig">DataProviderConfig</a></code></dt>
<dd>
<div class="desc"><p>Configuration settings for a specific provider
For example:
[1] S3 path + credentials
[2] BigQuery database URI + credentials
[3] DBT model directory + run command</p></div>
</dd>
</dl>
</dd>
<dt id="dataprovider.OutputDataDestination"><code class="flex name class">
<span>class <span class="ident">OutputDataDestination</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OutputDataDestination:
    &#34;&#34;&#34; &#34;&#34;&#34;

    # TODO: Add in the concept of a data sink.  How will we save things like prediction logs, cached training data, etc.</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dataprovider.DataProviderConfig" href="#dataprovider.DataProviderConfig">DataProviderConfig</a></code></h4>
</li>
<li>
<h4><code><a title="dataprovider.DataProviderType" href="#dataprovider.DataProviderType">DataProviderType</a></code></h4>
</li>
<li>
<h4><code><a title="dataprovider.InputDataSchema" href="#dataprovider.InputDataSchema">InputDataSchema</a></code></h4>
<ul class="">
<li><code><a title="dataprovider.InputDataSchema.data_checks" href="#dataprovider.InputDataSchema.data_checks">data_checks</a></code></li>
<li><code><a title="dataprovider.InputDataSchema.keys" href="#dataprovider.InputDataSchema.keys">keys</a></code></li>
<li><code><a title="dataprovider.InputDataSchema.metadata" href="#dataprovider.InputDataSchema.metadata">metadata</a></code></li>
<li><code><a title="dataprovider.InputDataSchema.output_features" href="#dataprovider.InputDataSchema.output_features">output_features</a></code></li>
<li><code><a title="dataprovider.InputDataSchema.timestamp" href="#dataprovider.InputDataSchema.timestamp">timestamp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dataprovider.InputDataSource" href="#dataprovider.InputDataSource">InputDataSource</a></code></h4>
<ul class="">
<li><code><a title="dataprovider.InputDataSource.environment" href="#dataprovider.InputDataSource.environment">environment</a></code></li>
<li><code><a title="dataprovider.InputDataSource.metadata" href="#dataprovider.InputDataSource.metadata">metadata</a></code></li>
<li><code><a title="dataprovider.InputDataSource.provider" href="#dataprovider.InputDataSource.provider">provider</a></code></li>
<li><code><a title="dataprovider.InputDataSource.provider_config" href="#dataprovider.InputDataSource.provider_config">provider_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="dataprovider.OutputDataDestination" href="#dataprovider.OutputDataDestination">OutputDataDestination</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>